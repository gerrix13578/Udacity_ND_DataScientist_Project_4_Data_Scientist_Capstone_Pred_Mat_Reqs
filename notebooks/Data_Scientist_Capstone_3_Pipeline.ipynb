{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA SCIENTIST CAPSTONE - PIPELINE & PREDICTION VALUES\n",
    "\n",
    "In this part we focus on how to predict the variable \"Time between Requests\" analysed before.\n",
    "As data to predict, we have these ones:\n",
    "- PLP id: Consumption Point identification\n",
    "- Material\n",
    "- Number of parts for each contanier in the Request - Assumed that each request needs only 1 container\n",
    "- Median of consumption parts per day in the Production process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD Data from Database\n",
    "\n",
    "The database is based in sql lite. This database is generated in the Python script Data_Scientist_Capstone_1_Prepare_and_Clean_Data.jpynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SFab GLin  UbiLínea      Material  Status  Denominacion Tipo Sum  Consumo  \\\n",
      "0   401   20  A07P2_NO  W05FF817157A     1.0  CIMBRA TECHO       NO  267.667   \n",
      "1   401   20  A07P2_NO  W05FF817157A     1.0  CIMBRA TECHO       NO  267.667   \n",
      "2   401   20  A07P2_NO  W05FF817157A     1.0  CIMBRA TECHO       NO  402.667   \n",
      "3   401   20  A07P2_NO  W05FF817157A     1.0  CIMBRA TECHO       NO  305.333   \n",
      "4   401   20  A07P2_NO  W05FF817157A     1.0  CIMBRA TECHO       NO  305.333   \n",
      "\n",
      "    F.Creac   H.Creac  ...         PLP      PVB            Válido de  \\\n",
      "0  29.09.25  20:34:08  ...  08AI_A07P2  08L2_AI  2000-01-01 00:00:00   \n",
      "1  30.09.25  09:12:00  ...  08AI_A07P2  08L2_AI  2000-01-01 00:00:00   \n",
      "2  01.10.25  00:51:19  ...  08AI_A07P2  08L2_AI  2000-01-01 00:00:00   \n",
      "3  02.10.25  06:12:09  ...  08AI_A07P2  08L2_AI  2000-01-01 00:00:00   \n",
      "4  02.10.25  20:32:03  ...  08AI_A07P2  08L2_AI  2000-01-01 00:00:00   \n",
      "\n",
      "              Válido a Cap. Sumin       datetime_creac        datetime_conf  \\\n",
      "0  9999-12-31 00:00:00      250.0  2025-09-29 20:34:08  2025-09-29 22:11:00   \n",
      "1  9999-12-31 00:00:00      250.0  2025-09-30 09:12:00  2025-09-30 10:35:00   \n",
      "2  9999-12-31 00:00:00      250.0  2025-10-01 00:51:19  2025-10-01 02:44:00   \n",
      "3  9999-12-31 00:00:00      250.0  2025-10-02 06:12:09  2025-10-02 08:24:00   \n",
      "4  9999-12-31 00:00:00      250.0  2025-10-02 20:32:03  2025-10-03 00:43:00   \n",
      "\n",
      "      Supply_time  Supply_time_hours time_between_MatReqs  \n",
      "0   5812000000000           1.614444            13.951944  \n",
      "1   4980000000000           1.383333            12.631111  \n",
      "2   6761000000000           1.878056            15.655278  \n",
      "3   7911000000000           2.197500            29.347222  \n",
      "4  15057000000000           4.182500            14.331667  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Database and table details\n",
    "db_name = \"SPA_Data_Analytics.db\"\n",
    "table_name = \"SPA_Historic_Manual_Requests\"\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_name)\n",
    "\n",
    "# Read the table into a DataFrame\n",
    "df_loaded = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_loaded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline 1\n",
    "\n",
    "First model to predict the variable \"Time between Reqs\" using the variables \"Number of parts for each contanier in the Request\", \"Median of consumption parts per day in the Production process\" and \"Supply Time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 11.94\n",
      "R² Score: 0.38\n"
     ]
    }
   ],
   "source": [
    "# Relevant columns\n",
    "features = ['Consumo', 'Cap. Sumin', 'Supply_time_hours'] # These variables are \"Median of consumption parts per day in the Production process\", \"Number of parts for each contanier in the Request\" and \"Supply Time\"\n",
    "target = 'time_between_MatReqs'\n",
    "\n",
    "X = df_loaded[features]\n",
    "y = df_loaded[target]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scale features\n",
    "    ('model', RandomForestRegressor(n_estimators=100, random_state=42))  # Regression model\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RandomForest RMSE: {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION Pipeline 1:\n",
    "\n",
    "The Results indicate:\n",
    "\n",
    "RMSE = 11.94 → The average prediction error is about 12 hours.\n",
    "R² = 0.38 → The model explains only 38% of the variance in Time between Requests\n",
    "\n",
    "This suggests the model is underperforming, likely due to:\n",
    "- Feature relevance: X varibales may not strongly correlate with Y.\n",
    "- Data size: the dataset could be small for this type of prediction, so the model may not generalize well.\n",
    "- Hyperparameters: Default RandomForestRegressor settings may not be optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline 2\n",
    "\n",
    "In this case, I try to improve the model using RandomForest and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF Params: {'model__max_depth': 10, 'model__min_samples_split': 5, 'model__n_estimators': 200}\n",
      "RandomForest -> RMSE: 11.38, R²: 0.44\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1. RandomForest with GridSearch\n",
    "# -------------------------------\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # optional for trees\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "rf_params = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [None, 10, 20],\n",
    "    'model__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(rf_pipeline, rf_params, cv=3, scoring='r2', n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best RF Params:\", rf_grid.best_params_)\n",
    "\n",
    "# Evaluate RF\n",
    "rf_pred = rf_grid.predict(X_test)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "rf_r2 = r2_score(y_test, rf_pred)\n",
    "\n",
    "print(f\"RandomForest -> RMSE: {rf_rmse:.2f}, R²: {rf_r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION Pipeline 2:\n",
    "\n",
    "Here we have an improvement with the RMSE and R² Parameters. So this model is better than the one done before to predict the \"Time between Requests variable\". \n",
    "This means the tuned RandomForest is capturing more variance and making slightly more accurate predictions. However, the performance is still modest — R² of 0.44 suggests the model explains less than half of the variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline 3\n",
    "\n",
    "Use of GradientBoosting for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoosting -> RMSE: 11.54, R²: 0.43\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# 2. GradientBoosting\n",
    "# -------------------------------\n",
    "gb_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # optional for boosting\n",
    "    ('model', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "gb_pred = gb_pipeline.predict(X_test)\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_pred))\n",
    "gb_r2 = r2_score(y_test, gb_pred)\n",
    "\n",
    "print(f\"GradientBoosting -> RMSE: {gb_rmse:.2f}, R²: {gb_r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION Pipeline 3:\n",
    "\n",
    "RandomForest still slightly outperforms GradientBoosting in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSIONS:\n",
    "\n",
    "Predict the Time between Requests it is not easier as it was expected: Depends hightly on the characteristics of each PLP (consumption point) as also seen in the analysis part of the project\n",
    "\n",
    "NEXT STEPS:\n",
    "\n",
    "Test with bigger amount of data (More than 4 weeks)\n",
    "Skip outliers on data: For example take in consideration that some requests are done just before weekend and the next one is after weekend when there is no production.\n",
    "Try to group the PLPs in groups with the same characteristics in order to build a better model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
